{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47a5125c",
   "metadata": {},
   "source": [
    "##### 1. Tokenising --> Chunking + Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4901d59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HillSeah\\Documents\\hill priv\\vector-db-research\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pypdf import PdfReader\n",
    "\n",
    "# ---------------------------\n",
    "# Setup logging\n",
    "# ---------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eada9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Text loading & chunking\n",
    "# ---------------------------\n",
    "def load_text_from_file(path: Path) -> str:\n",
    "    ext = path.suffix.lower()\n",
    "    if ext in [\".txt\", \".md\"]:\n",
    "        return path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    elif ext == \".pdf\":\n",
    "        text_parts = []\n",
    "        try:\n",
    "            reader = PdfReader(str(path))\n",
    "            for page in reader.pages:\n",
    "                # pypdf can return None for empty pages—guard it\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                text_parts.append(page_text)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to read PDF {path}: {e}\")\n",
    "        # document_parts = [\"Page 1 text...\", \"Page 2 text...\", \"Page 3 text...\"] AND \"/n\".join(document_parts) = \"Page 1 text...\\nPage 2 text...\\nPage 3 text...\"\n",
    "        return \"\\n\".join(text_parts)\n",
    "    else:\n",
    "        return \"\"  # unsupported\n",
    "\n",
    "\n",
    "def chunk_text(\n",
    "    text: str,\n",
    "    chunk_size: int,       # characters per chunk (roughly ~150-250 tokens)\n",
    "    chunk_overlap: int,    # characters of overlap\n",
    ") -> List[str]:\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    n = len(text)\n",
    "    while start < n:\n",
    "        end = min(start + chunk_size, n)\n",
    "        chunk = text[start:end]\n",
    "        # ensure we don’t cut off mid-word too badly by extending to next whitespace if possible\n",
    "        if end < n:\n",
    "            next_space = text.find(\" \", end)\n",
    "            if 0 < next_space - end < 20:  # small nudge to nearest space\n",
    "                chunk = text[start:next_space]\n",
    "                end = next_space\n",
    "        chunks.append(chunk.strip())\n",
    "        start = max(end - chunk_overlap, end)  # avoid infinite loop; no negative steps\n",
    "    # dedupe empties\n",
    "    return [c for c in chunks if c]\n",
    "\n",
    "\n",
    "def chunk_docs(\n",
    "    input_dir: Path,\n",
    "    chunk_size,       # characters per chunk (roughly ~150-250 tokens)\n",
    "    chunk_overlap,\n",
    ") -> List[Tuple[Path, List[str]]]:\n",
    "    docs = []\n",
    "    for path in input_dir.rglob(\"*\"):\n",
    "        if path.is_file() and path.suffix.lower() in {\".txt\", \".md\", \".pdf\"}:\n",
    "            text = load_text_from_file(path)\n",
    "            if not text:\n",
    "                continue\n",
    "            docs.append((path, chunk_text(text, chunk_size, chunk_overlap)))\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44496b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Embedding + Metadata\n",
    "# ---------------------------\n",
    "def collect_chunks_and_metadata(\n",
    "    input_dir,\n",
    "    chunk_size: int = 800,\n",
    "    chunk_overlap: int = 150,\n",
    ") -> Tuple[List[str], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Scan documents in input_dir, chunk them, and return both chunks and metadata.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Scanning input directory: {input_dir}\")\n",
    "    chunks, metadatas = [], []\n",
    "\n",
    "    for file_path, file_chunks in chunk_docs(input_dir, chunk_size, chunk_overlap):\n",
    "        for i, chunk in enumerate(file_chunks):\n",
    "            chunks.append(chunk)\n",
    "            metadatas.append({\n",
    "                \"source\": str(file_path),\n",
    "                \"chunk_id\": i,\n",
    "                \"preview\": chunk[:200].replace(\"\\n\", \" \") + (\"...\" if len(chunk) > 200 else \"\"),\n",
    "                \"full\": chunk\n",
    "            })\n",
    "\n",
    "    if not chunks:\n",
    "        raise ValueError(f\"No supported documents found in {input_dir}\")\n",
    "\n",
    "    logger.info(f\"Collected {len(chunks)} chunks from documents\")\n",
    "    return chunks, metadatas\n",
    "\n",
    "\n",
    "def embed_chunks(\n",
    "    chunks: List[str],\n",
    "    model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    batch_size: int = 64,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Embed text chunks using a SentenceTransformer model.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading embedding model: {model_name}\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    vectors = []\n",
    "    for i in tqdm(range(0, len(chunks), batch_size), desc=\"Embedding\"):\n",
    "        batch = chunks[i:i + batch_size]\n",
    "        vec = model.encode(batch, convert_to_numpy=True, normalize_embeddings=True)\n",
    "        vectors.append(vec)\n",
    "\n",
    "    X = np.vstack(vectors).astype(\"float32\")\n",
    "    logger.info(f\"Finished embedding. Shape: {X.shape}\")\n",
    "    return X\n",
    "\n",
    "\n",
    "def get_embeddings_and_metadata(\n",
    "    input_dir,\n",
    "    chunk_size: int = 800,\n",
    "    chunk_overlap: int = 150,\n",
    "    batch_size: int = 64,\n",
    "    model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Convenience wrapper: collects chunks+metadata, then embeds them.\n",
    "    \"\"\"\n",
    "    chunks, metadatas = collect_chunks_and_metadata(input_dir, chunk_size, chunk_overlap)\n",
    "    embeddings = embed_chunks(chunks, model_name=model_name, batch_size=batch_size)\n",
    "    return embeddings, metadatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fd47ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 11:52:06,916 [INFO] Scanning input directory: c:\\Users\\HillSeah\\Documents\\hill priv\\vector-db-research\\data\\kyndryl-docs-test\n",
      "2025-09-19 11:52:12,662 [WARNING] Ignoring wrong pointing object 7 0 (offset 0)\n",
      "2025-09-19 11:52:12,665 [WARNING] Ignoring wrong pointing object 18 0 (offset 0)\n",
      "2025-09-19 11:52:12,668 [WARNING] Ignoring wrong pointing object 20 0 (offset 0)\n",
      "2025-09-19 11:52:12,670 [WARNING] Ignoring wrong pointing object 59 0 (offset 0)\n",
      "2025-09-19 11:52:12,673 [WARNING] Ignoring wrong pointing object 119 0 (offset 0)\n",
      "2025-09-19 11:52:13,608 [INFO] Collected 350 chunks from documents\n",
      "2025-09-19 11:52:13,610 [INFO] Loading embedding model: sentence-transformers/all-mpnet-base-v2\n",
      "2025-09-19 11:52:13,626 [INFO] Use pytorch device_name: cpu\n",
      "2025-09-19 11:52:13,629 [INFO] Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "Batches: 100%|██████████| 2/2 [00:15<00:00,  7.90s/it]\n",
      "Batches: 100%|██████████| 2/2 [00:09<00:00,  4.99s/it]t]\n",
      "Batches: 100%|██████████| 2/2 [00:14<00:00,  7.30s/it]t]\n",
      "Batches: 100%|██████████| 2/2 [00:12<00:00,  6.33s/it]t]\n",
      "Batches: 100%|██████████| 2/2 [00:16<00:00,  8.28s/it]t]\n",
      "Batches: 100%|██████████| 1/1 [00:04<00:00,  4.59s/it]t]\n",
      "Embedding: 100%|██████████| 6/6 [01:14<00:00, 12.40s/it]\n",
      "2025-09-19 11:53:34,830 [INFO] Finished embedding. Shape: (350, 768)\n"
     ]
    }
   ],
   "source": [
    "# config\n",
    "input_dir = Path().cwd().parent /'data' / 'kyndryl-docs-test'\n",
    "model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "chunk_size = 200     # characters per chunk (roughly ~150-250 tokens)\n",
    "chunk_overlap = 50    # characters of overlap\n",
    "batch_size = 64 # batch building of index\n",
    "\n",
    "embeddings, metadatas = get_embeddings_and_metadata(\n",
    "    input_dir=input_dir,\n",
    "    model_name=model_name,\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b53c982f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(350, 768)\n",
      "350\n"
     ]
    }
   ],
   "source": [
    "# (350, 384) --> 350 chunks/vector embeddings; 384 = embedding dimension (columns) of the embedding model, output = (1, 384) per chunk\n",
    "print(embeddings.shape)\n",
    "\n",
    "# 350 metadata dictionaries for 350 chunks/vector embeddings\n",
    "print(len(metadatas))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008fdcf5",
   "metadata": {},
   "source": [
    "##### 2. Vector indexing and storage with Milvus\n",
    "\n",
    "- Define schema (collections, fields, vector dimensions, etc.)\n",
    "- Create indices on vector embeddings\n",
    "    - Define index type, e.g., HNSW, IVF_FLAT, or IVFPQ\n",
    "    - Define similarity metric (used both for 1. building the index and 2. for similarity search at retrieval time, e.g., L2, IP, cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc98f3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "\n",
    "\n",
    "def build_collection_name(dataset, index_type, similarity_metric_type, hyperparameters):\n",
    "    \n",
    "    collection_name  = dataset + \"_\" + index_type + \"_\" + similarity_metric_type\n",
    "    for k, v in hyperparameters.items():\n",
    "        collection_name += f\"_{k}_{v}\"\n",
    "        \n",
    "    return collection_name\n",
    "\n",
    "\n",
    "def get_milvus_connection(\n",
    "    alias: str = \"default\",\n",
    "    host: str = \"localhost\",\n",
    "    port: str = \"19530\",\n",
    "    reset: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Get or create a Milvus connection.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    alias : str\n",
    "        Connection alias (default: \"default\").\n",
    "    host : str\n",
    "        Milvus host.\n",
    "    port : str\n",
    "        Milvus port.\n",
    "    reset : bool\n",
    "        If True, disconnect existing connection and reconnect.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    alias : str\n",
    "        The alias name of the connection.\n",
    "    \"\"\"\n",
    "    if reset:\n",
    "        connections.disconnect(alias)\n",
    "        logger.info(\"Disconnected from Milvus Connection (RESET)\")\n",
    "\n",
    "    if not any(c[0] == alias and c[1] for c in connections.list_connections()):\n",
    "        connections.connect(alias=alias, host=host, port=port)\n",
    "        logger.info(f\"Successfully Milvus connection on Host: {host}, Port: {port}\")\n",
    "        \n",
    "    return alias\n",
    "\n",
    "\n",
    "def close_milvus_connection(alias: str = \"default\"):\n",
    "    \"\"\"\n",
    "    Disconnect from Milvus explicitly.\n",
    "    \"\"\"\n",
    "    if any(c[0] == alias and c[1] for c in connections.list_connections()):\n",
    "        connections.disconnect(alias)\n",
    "        logger.info(\"Disconnected from Milvus Connection\")\n",
    "    \n",
    "# ---------------------------\n",
    "# Build Milvus index\n",
    "# ---------------------------\n",
    "def build_milvus_index(\n",
    "    embeddings: np.ndarray,\n",
    "    metadatas: List,\n",
    "    similarity_metric_type: str,\n",
    "    index_type: str,\n",
    "    hyperparameters: Dict,\n",
    "    collection_name: str = \"document_embeddings\",\n",
    "    alias:str = \"default\",\n",
    "    host: str = \"localhost\",\n",
    "    port: str = \"19530\",\n",
    "):\n",
    "    # 1) Connect to Milvus\n",
    "    logger.info(f\"Connecting to Milvus at {host}:{port} ...\")\n",
    "    get_milvus_connection(alias=alias, host=host, port=port)\n",
    "   \n",
    "    # 2) Define Milvus collection schema if not exists\n",
    "    dim = embeddings.shape[1]\n",
    "    if utility.has_collection(collection_name):\n",
    "        logger.info(f\"Dropping existing collection: {collection_name}\")\n",
    "        utility.drop_collection(collection_name)\n",
    "\n",
    "    logger.info(f\"Creating new collection: {collection_name}\")\n",
    "    fields = [\n",
    "        FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "        FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=dim),\n",
    "        FieldSchema(name=\"source\", dtype=DataType.VARCHAR, max_length=500),\n",
    "        FieldSchema(name=\"chunk_id\", dtype=DataType.INT64),\n",
    "        FieldSchema(name=\"preview\", dtype=DataType.VARCHAR, max_length=500),\n",
    "        FieldSchema(name=\"full\", dtype=DataType.VARCHAR, max_length=65535), \n",
    "    ]\n",
    "    schema = CollectionSchema(fields, description=\"Document embeddings\")\n",
    "    collection = Collection(name=collection_name, schema=schema)\n",
    "\n",
    "    # 6) Insert data (dictionary-style, order-independent)\n",
    "    logger.info(f\"Inserting {len(metadatas)} records into Milvus...\")\n",
    "    insert_data = [\n",
    "        {\n",
    "            \"embedding\": embeddings[i].tolist(),\n",
    "            \"source\": metadatas[i][\"source\"],\n",
    "            \"chunk_id\": metadatas[i][\"chunk_id\"],\n",
    "            \"preview\": metadatas[i][\"preview\"],\n",
    "            \"full\": metadatas[i][\"full\"],\n",
    "        }\n",
    "        for i in range(len(metadatas))\n",
    "    ]\n",
    "    collection.insert(insert_data)\n",
    "    logger.info(\"Insertion complete.\")\n",
    "\n",
    "    # 7) Create index on vector field (for search efficiency)\n",
    "    logger.info(\"Creating index on `embedding` field...\")\n",
    "    index_params = {\n",
    "        \"metric_type\": similarity_metric_type,  # cosine similarity since embeddings are normalized\n",
    "        \"index_type\": index_type,\n",
    "        \"params\": hyperparameters # index-specific hyperparameters: e.g. nlist for IVF-based indices, M and efConstruction for HNSW\n",
    "    }\n",
    "    collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "\n",
    "    collection.load()\n",
    "    logger.info(\n",
    "        f\"[OK] Inserted {embeddings.shape[0]} vectors into Milvus collection `{collection_name}` \"\n",
    "        f\"(dim={dim}, index type = {index_type}, similarity metric type = {similarity_metric_type}).\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3f4efd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 13:50:20,429 [INFO] Connecting to Milvus at localhost:19530 ...\n",
      "2025-09-19 13:50:20,470 [INFO] Creating new collection: kyndryl_pdfs_IVF_PQ_IP_nlist_1_m_16_nbits_8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 13:50:20,925 [INFO] Inserting 350 records into Milvus...\n",
      "2025-09-19 13:50:21,264 [INFO] Insertion complete.\n",
      "2025-09-19 13:50:21,266 [INFO] Creating index on `embedding` field...\n",
      "2025-09-19 13:50:22,697 [INFO] [OK] Inserted 350 vectors into Milvus collection `kyndryl_pdfs_IVF_PQ_IP_nlist_1_m_16_nbits_8` (dim=768, index type = IVF_PQ, similarity metric type = IP).\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "similarity_metric_type = \"IP\"\n",
    "index_type = \"IVF_PQ\"\n",
    "hyperparameters = {\n",
    "    \"nlist\": 1, # nlist=1 to simulate Flat exhaustive search\n",
    "    \"m\": 16, # m=subvectors\n",
    "    \"nbits\": 8 # nbits=bits per subvector\n",
    "}\n",
    "host = \"localhost\"\n",
    "port = \"19530\"\n",
    "\n",
    "dataset_name = \"kyndryl_pdfs\"\n",
    "collection_name = build_collection_name(dataset=dataset_name, index_type=index_type, similarity_metric_type=similarity_metric_type, hyperparameters=hyperparameters)\n",
    "\n",
    "# Run -- ensure milvus docker containers are running: >> docker compose up -d\n",
    "build_milvus_index(\n",
    "    embeddings=embeddings,\n",
    "    metadatas=metadatas,\n",
    "    similarity_metric_type=similarity_metric_type,\n",
    "    index_type=index_type,\n",
    "    hyperparameters=hyperparameters,\n",
    "    collection_name=collection_name,\n",
    "    host=host,\n",
    "    port=port,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a239b0",
   "metadata": {},
   "source": [
    "#### 3. Vector Search, Top K (ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2234ec65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Dict, Optional\n",
    "from pymilvus import Collection\n",
    "from sentence_transformers import SentenceTransformer  # example embedding model\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def search_milvus(\n",
    "    queries: List[str],\n",
    "    embedding_model: str = \"all-MiniLM-L6-v2\",\n",
    "    similarity_metric_type: str = \"COSINE\",\n",
    "    index_search_params: Optional[Dict] = None,\n",
    "    top_k: int = 5,\n",
    "    alias: str = \"default\",\n",
    "    host: str = \"localhost\",\n",
    "    port: str = \"19530\",\n",
    "    collection_name: str = \"document_embeddings\"\n",
    ") -> List[List[Dict]]:\n",
    "    \"\"\"\n",
    "    Perform similarity search in Milvus and return top-k results with metadata.\n",
    "\n",
    "    Args:\n",
    "        queries: List of raw query strings\n",
    "        embedding_model: Name of the embedding model (e.g., from sentence-transformers)\n",
    "        similarity_metric_type: \"COSINE\", \"L2\", or \"IP\"\n",
    "        index_search_params: Index-specific search parameters (e.g., {\"ef\": 200} for HNSW, {\"nprobe\": 10} for IVF)\n",
    "        top_k: Number of top results to return\n",
    "        alias: Alias of targeted Milvus database to search\n",
    "        host: Host of targeted Milvus database to search\n",
    "        port: Port of targeted Milvus database to search\n",
    "        collection_name: Name of the Milvus collection\n",
    "\n",
    "    Returns:\n",
    "        List of results per query, each as a list of dicts containing 'score' and metadata fields\n",
    "    \"\"\"\n",
    "    # Connect to Milvus (if not already connected)\n",
    "    get_milvus_connection(alias=alias, host=host, port=port)\n",
    "\n",
    "    # Generate embeddings for queries\n",
    "    logger.info(f\"Encoding {len(queries)} queries using {embedding_model} ...\")\n",
    "    model = SentenceTransformer(embedding_model)\n",
    "    query_embeddings = model.encode(queries, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "    # Load the collection\n",
    "    collection = Collection(collection_name)\n",
    "    collection.load()\n",
    "    logger.info(f\"Loaded collection {collection_name} | Host: {host} | Port: {port}...\")\n",
    "\n",
    "    if index_search_params is None:\n",
    "        index_search_params = {}\n",
    "\n",
    "    results = collection.search(\n",
    "        data=query_embeddings.tolist(),\n",
    "        anns_field=\"embedding\", \n",
    "        param=index_search_params,\n",
    "        limit=top_k,\n",
    "        metric_type=similarity_metric_type,\n",
    "        output_fields=[\"source\", \"chunk_id\", \"preview\", \"full\"]\n",
    "    )\n",
    "\n",
    "    all_results = []\n",
    "    for i, hits in enumerate(results):\n",
    "        query_results = []\n",
    "        for hit in hits:\n",
    "            query_results.append({\n",
    "                \"id\": hit.id,\n",
    "                \"score\": hit.score,\n",
    "                \"source\": hit.entity.get(\"source\"),\n",
    "                \"chunk_id\": hit.entity.get(\"chunk_id\"),\n",
    "                \"preview\": hit.entity.get(\"preview\"),\n",
    "                \"full\": hit.entity.get(\"full\"),\n",
    "                \"query\": queries[i]\n",
    "            })\n",
    "        all_results.append(query_results)\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "918982b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 13:52:42,136 [INFO] Encoding 1 queries using sentence-transformers/all-mpnet-base-v2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 13:52:42,158 [INFO] Use pytorch device_name: cpu\n",
      "2025-09-19 13:52:42,160 [INFO] Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "index_search_params = None\n",
    "top_k = 5\n",
    "\n",
    "# Same as during storing in target collection\n",
    "embedding_model = 'sentence-transformers/all-mpnet-base-v2'\n",
    "similarity_metric_type = \"IP\"\n",
    "alias = \"default\"\n",
    "host = \"localhost\"\n",
    "port = \"19530\"\n",
    "dataset_name = \"kyndryl_pdfs\"\n",
    "collection_name = build_collection_name(dataset=dataset_name, index_type=index_type, similarity_metric_type=similarity_metric_type, hyperparameters=hyperparameters)\n",
    "\n",
    "# Query\n",
    "queries = [\n",
    "    \"How much does Kyndryl cover for surgeries\"\n",
    "]\n",
    "\n",
    "# Main\n",
    "all_results = search_milvus(\n",
    "    queries=queries,\n",
    "    embedding_model=embedding_model,\n",
    "    similarity_metric_type=similarity_metric_type,\n",
    "    index_search_params=index_search_params,\n",
    "    top_k=top_k,\n",
    "    alias=alias,\n",
    "    host=host,\n",
    "    port=port,\n",
    "    collection_name=collection_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c199ca0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Query 1: How much does Kyndryl cover for surgeries\n",
      "    [0.6114] Yes, you could but the hospital might impose an administration fee. The administration fee is not payable under Kyndryl medical program.  3. Q: If I am covered under Kyndryl’s medical insurance polici...\n",
      "    [0.5777] 101.42 Dependant 99.83 Deluxe (Opt-up) Employee 209.17 Dependant 204.83 Note:  ▪ Kyndryl’s 2025 price tags have been adjusted to reflect the rising cost of medical treatment.  ▪ Kyndryl’s flex benefit...\n",
      "    [0.5753] Kyndryl  funds these for  employees) Policy No. 79343 53015577 79343 Eligibility All Employees All Employees All Employees & their Eligible Dependants Basis of Cover • All Employees: 2 times  annual g...\n",
      "    [0.5571] surgery is covered *S$35,000 Surgical benefits are Up to 50%  of max. limit Surgical schedule applies to limit  above S$1,500 for surgery in  private hospitals Day surgery is covered 3 Surgical Benefi...\n",
      "    [0.5461] for surgery in private  hospitals Day surgery is covered *$25,000 Surgical benefits are Up to 50% of  max. limit Surgical schedule applies to limit  above S$1,500 for surgery in private  hospitals Day\n"
     ]
    }
   ],
   "source": [
    "for i, results in enumerate(all_results):\n",
    "    print(f\"Results for Query {i+1}: {queries[i]}\")\n",
    "    for r in results:\n",
    "        print(f\"    [{r['score']:.4f}] {r['preview']}\")\n",
    "        \n",
    "# super interesting (or not really)\n",
    "# Results and scores were exactly the same as FAISS (Same similarity metric and index, IP + Flat)\n",
    "\n",
    "# (1) sentence-transformers/all-MiniLM-L6-v2: \n",
    "# Results for Query 1: How much does Kyndryl cover for surgeries\n",
    "    # [0.6130] Yes, you could but the hospital might impose an administration fee. The administration fee is not payable under Kyndryl medical program.  3. Q: If I am covered under Kyndryl’s medical insurance polici...\n",
    "    # [0.5646] 101.42 Dependant 99.83 Deluxe (Opt-up) Employee 209.17 Dependant 204.83 Note:  ▪ Kyndryl’s 2025 price tags have been adjusted to reflect the rising cost of medical treatment.  ▪ Kyndryl’s flex benefit...\n",
    "    # [0.5614] top of the bill.  There must not be any  outstanding amount due. 38 KYNDRYL FUNDED  PROGRAMS 38  39 Kyndryl Funded Benefits ▪ Applicable to dependants enrolled under Insured Medical Plan ▪ This progra...\n",
    "    # [0.5316] Kyndryl  funds these for  employees) Policy No. 79343 53015577 79343 Eligibility All Employees All Employees All Employees & their Eligible Dependants Basis of Cover • All Employees: 2 times  annual g...\n",
    "    # [0.4904] surgery is covered *S$35,000 Surgical benefits are Up to 50%  of max. limit Surgical schedule applies to limit  above S$1,500 for surgery in  private hospitals Day surgery is covered 3 Surgical Benefi..\n",
    "\n",
    "# (2) 'sentence-transformers/all-mpnet-base-v2'\n",
    "# Results for Query 1: How much does Kyndryl cover for surgeries\n",
    "#     [0.6114] Yes, you could but the hospital might impose an administration fee. The administration fee is not payable under Kyndryl medical program.  3. Q: If I am covered under Kyndryl’s medical insurance polici...\n",
    "#     [0.5777] 101.42 Dependant 99.83 Deluxe (Opt-up) Employee 209.17 Dependant 204.83 Note:  ▪ Kyndryl’s 2025 price tags have been adjusted to reflect the rising cost of medical treatment.  ▪ Kyndryl’s flex benefit...\n",
    "#     [0.5753] Kyndryl  funds these for  employees) Policy No. 79343 53015577 79343 Eligibility All Employees All Employees All Employees & their Eligible Dependants Basis of Cover • All Employees: 2 times  annual g...\n",
    "#     [0.5571] surgery is covered *S$35,000 Surgical benefits are Up to 50%  of max. limit Surgical schedule applies to limit  above S$1,500 for surgery in  private hospitals Day surgery is covered 3 Surgical Benefi...\n",
    "#     [0.5461] for surgery in private  hospitals Day surgery is covered *$25,000 Surgical benefits are Up to 50% of  max. limit Surgical schedule applies to limit  above S$1,500 for surgery in private  hospitals Day"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vector-db-research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
